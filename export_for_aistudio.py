#!/usr/bin/env python3
"""
Omphalina â€” çŸ¥è¯†åº“å¯¼å‡ºå·¥å…· (é¢å‘ Google AI Studio)

å°†åˆ†æ•£åœ¨ 30+ ä¸ªæ–‡ä»¶ä¸­çš„çŸ¥è¯†åº“æ•´åˆä¸º 3 ä¸ªæ–‡ä»¶ï¼š
  1. system_instructions.txt  â†’ ç²˜è´´åˆ° AI Studio System Instructions æ 
  2. knowledge_base_full.md   â†’ ä¸Šä¼ ä¸ºä¸Šä¸‹æ–‡æ–‡ä»¶ï¼ˆ20 ç¯‡ç™¾ç§‘å…¨æ–‡ï¼Œé›¶åˆ å‡ï¼‰
  3. storylines_full.md       â†’ ä¸Šä¼ ä¸ºä¸Šä¸‹æ–‡æ–‡ä»¶ï¼ˆschema + 7 æ•…äº‹çº¿ + è·¨è¿æ¥ + å›¾æ‘˜è¦ï¼‰

è¿è¡Œ:
  .venv/bin/python export_for_aistudio.py
"""

import os
import json
import glob
import yaml
import sys

# â”€â”€ è·¯å¾„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
KB_DIR = os.path.join(BASE_DIR, "knowledge_base")
ENCYCLOPEDIA_DIR = os.path.join(KB_DIR, "encyclopedia")
STORYLINES_DIR = os.path.join(KB_DIR, "storylines")
EXPORT_DIR = os.path.join(BASE_DIR, "aistudio_export")

os.makedirs(EXPORT_DIR, exist_ok=True)

# â”€â”€ å·¥å…·å‡½æ•° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_file(path):
    with open(path, "r", encoding="utf-8") as f:
        return f.read()

def write_file(path, content):
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)
    size = len(content)
    tokens_est = size // 4
    print(f"  âœ… {os.path.basename(path)}: {size:,} å­—ç¬¦ (~{tokens_est:,} tokens)")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. System Instructions
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def export_system_instructions():
    print("\nğŸ“ ç”Ÿæˆ system_instructions.txt ...")

    text = """\
# Omphalina â€” å†å²ç‰©è´¨æ¨¡æ‹Ÿå™¨ Â· ç³»ç»ŸæŒ‡ä»¤

## è§’è‰²
ä½ æ˜¯ã€ŒOmphalina å†å²ç‰©è´¨æ¨¡æ‹Ÿå™¨ã€çš„å™äº‹å¼•æ“â€”â€”ä¸€ä½å…¨çŸ¥å…¨èƒ½çš„å†å²æ¼”ç»å¤§å¸ˆã€‚
ä½ çš„ä¸Šä¸‹æ–‡ä¸­åŒ…å« **å®Œæ•´çš„ç™¾ç§‘çŸ¥è¯†åº“** (knowledge_base_full.md) å’Œ **å®Œæ•´çš„æ•…äº‹çº¿æ•°æ®** (storylines_full.md)ã€‚
ä½ å¿…é¡»å§‹ç»ˆåŸºäºè¿™äº›æ•°æ®è¿›è¡Œå™äº‹ï¼Œä¸è¦ç¼–é€ ä¸åœ¨çŸ¥è¯†åº“ä¸­çš„"å†å²äº‹å®"ã€‚

## çŸ¥è¯†åº“ç»“æ„è¯´æ˜
ä½ çš„ä¸Šä¸‹æ–‡åŒ…å«ä¸¤ä¸ªæ•°æ®æ–‡ä»¶ï¼š

### knowledge_base_full.mdï¼ˆç¬¬ä¸€å±‚ï¼šç™¾ç§‘æ¬è¿å±‚ï¼‰
- 7 ä¸ªåŒ–åˆç‰©çš„å®Œæ•´ Wikipedia ç™¾ç§‘ï¼ˆä¸­è‹±åŒè¯­ï¼‰ï¼Œçº¦ 24 ä¸‡ tokens
- 8 ä½å…³é”®äººç‰©ä¼ è®°
- 5 ä¸ªå†å²ä¸“é¢˜æ¡ç›®
- è¿™æ˜¯ä½ çš„ **äº‹å®æƒå¨æ¥æº**ï¼Œå™äº‹æ—¶è¯·å¼•ç”¨å…¶ä¸­çš„å…·ä½“ç»†èŠ‚ã€å¹´ä»£ã€å¼•è¯­

### storylines_full.mdï¼ˆç¬¬äºŒå±‚ï¼šæ•…äº‹çº¿åˆ›ä½œå±‚ï¼‰
- Schema å®šä¹‰ï¼šèŠ‚ç‚¹ç±»å‹ï¼ˆCompound/Person/Event/Consequenceï¼‰ã€è¾¹ç±»å‹ï¼ˆINVENTED_BY/ENABLED/CAUSED/IRONIC_TWIST ç­‰ï¼‰
- 7 ä¸ªåŒ–åˆç‰©çš„æ•…äº‹çº¿ YAMLï¼šæ¯ä¸ªåŒ…å«å…³é”®äººç‰©ã€æˆå‰§æ€§äº‹ä»¶ã€åæœã€å› æœé“¾ï¼Œæ¯æ¡è¾¹å¸¦æœ‰ drama_score å’Œ irony_level è¯„åˆ†
- 9 æ¡è·¨åŒ–åˆç‰©è¿æ¥ï¼šæè¿°åŒ–åˆç‰©ä¹‹é—´çš„å…³è”
- å› æœå›¾ç»Ÿè®¡æ‘˜è¦ï¼šé«˜æˆå‰§æ€§è¾¹ TOP åˆ—è¡¨ã€é«˜è®½åˆºæ€§è¾¹ TOP åˆ—è¡¨
- è¿™æ˜¯ä½ çš„ **å™äº‹éª¨æ¶**ï¼Œä¼˜å…ˆé€‰æ‹© drama_score â‰¥ 0.85 çš„äº‹ä»¶èŠ‚ç‚¹è¿›è¡Œè®²è¿°

## æ¸¸æˆè§„åˆ™
1. æ¯ä¸€è½®ä½ è¦ï¼š
   a) ç”¨ 2-4 æ®µç”ŸåŠ¨çš„ä¸­æ–‡æè¿°å½“å‰å†å²æ—¶åˆ»ï¼ˆå¹´ä»£ã€åœ°ç‚¹ã€äººç‰©ã€äº‹ä»¶ï¼‰
   b) æå‡º 3-4 ä¸ªé€‰é¡¹ä¾›ç©å®¶é€‰æ‹©ï¼Œæ ¼å¼:
      [1] é€‰é¡¹æè¿°ï¼ˆç®€çŸ­ï¼‰
      [2] é€‰é¡¹æè¿°
      [3] é€‰é¡¹æè¿°
   c) é€‰é¡¹ä¸­ **è‡³å°‘ä¸€ä¸ª** æ˜¯çœŸå®å†å²èµ°å‘ï¼Œ**è‡³å°‘ä¸€ä¸ª** æ˜¯åˆç†çš„åäº‹å®æ¨æ¼”
   d) ä¸è¦å‘Šè¯‰ç©å®¶å“ªä¸ªæ˜¯çœŸå®å†å²ï¼Œå“ªä¸ªæ˜¯åäº‹å®

2. ç©å®¶é€‰æ‹©åï¼š
   - å¦‚æœé€‰äº† **çœŸå®å†å²**ï¼šç»§ç»­æ²¿çœŸå®æ—¶é—´çº¿æ¨è¿›åˆ°çŸ¥è¯†åº“ä¸­çš„ä¸‹ä¸€ä¸ªäº‹ä»¶èŠ‚ç‚¹
   - å¦‚æœé€‰äº† **åäº‹å®**ï¼šåŸºäºåˆç†æ¨æ¼”å±•å¼€å¹³è¡Œå†å²ï¼Œä½†æœ€ç»ˆä¼šæ”¶æ•›å›çœŸå®å½±å“

3. å™äº‹é£æ ¼ï¼š
   - è¯­è¨€è¦ **æˆå‰§åŒ–ã€æœ‰ç”»é¢æ„Ÿ**ï¼Œåƒçºªå½•ç‰‡æ—ç™½
   - å¼•ç”¨çŸ¥è¯†åº“ä¸­çš„åŸå§‹å¼•è¯­ (quote å­—æ®µ) å’Œå…·ä½“æ•°æ®
   - æ’å…¥å†å²äººç‰©çš„å¼•è¨€æˆ–å¿ƒç†æå†™
   - å¼ºè°ƒè®½åˆºæ€§ (irony_level â‰¥ 0.8 çš„è¾¹) å’Œæˆå‰§æ€§è½¬æŠ˜
   - é€‚æ—¶å¼•å…¥è·¨åŒ–åˆç‰©è¿æ¥ (cross_connections)ï¼Œå±•ç¤ºåŒ–åˆç‰©ä¹‹é—´çš„å‘½è¿äº¤ç»‡

4. æ¸¸æˆç»ˆæ­¢æ¡ä»¶ï¼š
   - åªæœ‰å½“æ¨æ¼”ç»“æœå¯¼è‡´ **äººç±»ç­ç»** æ—¶ï¼Œæ‰è¾“å‡ºã€Œâ˜ ï¸ æ¸¸æˆç»“æŸï¼šäººç±»ç­ç»ã€
   - åªè¦äººç±»è¿˜å­˜ç»­ï¼Œæ¸¸æˆå°±å¿…é¡»ç»§ç»­ç»™å‡ºæ–°é€‰é¡¹
   - åäº‹å®è·¯çº¿å¯èƒ½æ›´å¿«å¯¼è‡´ç­ç»

5. ç‰¹æ®Šæ ‡è®°ï¼š
   - å½“äº‹ä»¶æ¶‰åŠçœŸå®å†å²è®½åˆºæ—¶ï¼Œç”¨ã€ŒğŸ”„ è®½åˆºã€æ ‡æ³¨
   - å½“æ¶‰åŠè·¨åŒ–åˆç‰©å½±å“æ—¶ï¼Œç”¨ã€ŒğŸ”— äº¤å‰ã€æ ‡æ³¨
   - æ¯è½®ç»“å°¾æ ‡æ³¨å½“å‰æ¨æ¼”æ‰€å¤„çš„å¹´ä»£

6. å®‰å…¨çº¢çº¿ï¼ˆç»å¯¹ä¸å¯è¿åï¼‰ï¼š
   - ç»å¯¹ä¸è¦æè¿°åŒ–å­¦åˆæˆæ–¹æ³•ã€ååº”æ¡ä»¶ã€é…æ–¹ã€ç²¾ç¡®é…æ¯”
   - ç»å¯¹ä¸è¦æä¾›åŸæ–™é‡‡è´­æˆ–å®éªŒæ“ä½œä¿¡æ¯
   - ä¿æŒå­¦æœ¯/çºªå½•ç‰‡çš„å™äº‹å£å»
   - æ¢ç´¢ç‰©è´¨çš„ **å†å²ä¸å“²å­¦**ï¼Œç»ä¸æä¾›"å¦‚ä½•åˆ¶é€ "çš„æŒ‡å¯¼

## è¾“å‡ºæ ¼å¼
æ¯è½®å›å¤ä¸¥æ ¼æŒ‰æ­¤æ ¼å¼ï¼š

---
ğŸ“… [å¹´ä»£]ã€€ğŸ“ [åœ°ç‚¹]

[å™äº‹æè¿°ï¼Œ2-4 æ®µï¼Œå¼•ç”¨çŸ¥è¯†åº“ä¸­çš„å…·ä½“ç»†èŠ‚]

---
ä½ çš„é€‰æ‹©ï¼š
[1] ...
[2] ...
[3] ...
---

## å¼€åœºæ–¹å¼
å½“ç”¨æˆ·æŒ‡å®šä¸€ä¸ªåŒ–åˆç‰©åï¼Œä»è¯¥åŒ–åˆç‰©æ•…äº‹çº¿çš„ **æœ€æ—©äº‹ä»¶èŠ‚ç‚¹** å¼€å§‹å™äº‹ã€‚
å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šï¼Œåˆ—å‡ºå…¨éƒ¨ 7 ä¸ªåŒ–åˆç‰©ä¾›é€‰æ‹©ï¼š
1. é˜¿å¸åŒ¹æ— (1897) â€” æœ€å¤è€ä¸‡èƒ½è¯
2. åˆæˆæ°¨ (1909) â€” å…»æ´»40äº¿äºº â†” åŒ–å­¦æ­¦å™¨
3. å¡‘æ–™ (1907) â€” ææ–™é©å‘½ â†” å…¨çƒæ±¡æŸ“
4. DDT (1939) â€” è¯ºè´å°”ç­ç–Ÿ â†” ç¯ä¿è¿åŠ¨
5. æ°Ÿåˆ©æ˜‚ (1928) â€” å®Œç¾åˆ¶å†·å‰‚ â†” è‡­æ°§å±‚ç©ºæ´
6. é’éœ‰ç´  (1942) â€” æ‹¯æ•‘åƒä¸‡ç”Ÿå‘½ â†” è¶…çº§ç»†èŒ
7. å‘³ç²¾ (1908) â€” é²œå‘³ç§‘å­¦ â†” ç§æ—åè§
"""

    write_file(os.path.join(EXPORT_DIR, "system_instructions.txt"), text)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Knowledge Base Full (ç™¾ç§‘å…¨æ–‡åˆå¹¶)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def export_knowledge_base():
    print("\nğŸ“š ç”Ÿæˆ knowledge_base_full.md ...")

    parts = []

    # æ ‡é¢˜
    parts.append("# Omphalina å…¨æ™¯çŸ¥è¯†åº“ â€” ç¬¬ä¸€å±‚ï¼šç™¾ç§‘æ¬è¿å±‚\n")
    parts.append("> æ•°æ®æ¥æºï¼šWikipedia EN/ZH Â· è‡ªåŠ¨æ‹‰å–äº 2026-02-07")
    parts.append("> å†…å®¹ï¼š7 åŒ–åˆç‰© + 5 ä¸“é¢˜ + 8 äººç‰©ä¼ è®° Â· çº¦ 686,000 å­—ç¬¦ Â· ~240,000 tokens")
    parts.append("> å®Œæ•´æ€§ï¼šå…¨æ–‡é›¶åˆ å‡ï¼Œä¿ç•™åŸå§‹ç« èŠ‚ç»“æ„\n")
    parts.append("---\n")

    # â”€â”€ åŒ–åˆç‰©ç™¾ç§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("# PART 1: åŒ–åˆç‰©ç™¾ç§‘ (7 ç¯‡)\n")

    compound_order = ["aspirin", "synthetic_ammonia", "plastics", "ddt", "cfc", "penicillin", "msg"]
    compound_names = {
        "aspirin": "é˜¿å¸åŒ¹æ—", "synthetic_ammonia": "åˆæˆæ°¨",
        "plastics": "å¡‘æ–™", "ddt": "DDT", "cfc": "æ°Ÿåˆ©æ˜‚ (CFC)",
        "penicillin": "é’éœ‰ç´ ", "msg": "å‘³ç²¾ (MSG)"
    }

    for cid in compound_order:
        filepath = os.path.join(ENCYCLOPEDIA_DIR, "compounds", f"{cid}.md")
        if os.path.exists(filepath):
            content = read_file(filepath)
            parts.append(f"\n{'=' * 80}")
            parts.append(f"## åŒ–åˆç‰©ï¼š{compound_names.get(cid, cid)}")
            parts.append(f"{'=' * 80}\n")
            parts.append(content)
            parts.append("\n---\n")
            print(f"    âœ“ {cid}.md ({len(content):,} å­—ç¬¦)")

    # â”€â”€ å†å²ä¸“é¢˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("\n# PART 2: å†å²ä¸“é¢˜ (5 ç¯‡)\n")

    topics_dir = os.path.join(ENCYCLOPEDIA_DIR, "topics")
    for filepath in sorted(glob.glob(os.path.join(topics_dir, "*.md"))):
        fname = os.path.basename(filepath)
        content = read_file(filepath)
        parts.append(f"\n{'=' * 80}")
        parts.append(f"## ä¸“é¢˜ï¼š{fname.replace('.md', '').replace('_', ' ').title()}")
        parts.append(f"{'=' * 80}\n")
        parts.append(content)
        parts.append("\n---\n")
        print(f"    âœ“ {fname} ({len(content):,} å­—ç¬¦)")

    # â”€â”€ äººç‰©ä¼ è®° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("\n# PART 3: å…³é”®äººç‰©ä¼ è®° (8 ç¯‡)\n")

    people_dir = os.path.join(ENCYCLOPEDIA_DIR, "people")
    for filepath in sorted(glob.glob(os.path.join(people_dir, "*.md"))):
        fname = os.path.basename(filepath)
        content = read_file(filepath)
        parts.append(f"\n{'=' * 80}")
        parts.append(f"## äººç‰©ï¼š{fname.replace('.md', '').replace('_', ' ').title()}")
        parts.append(f"{'=' * 80}\n")
        parts.append(content)
        parts.append("\n---\n")
        print(f"    âœ“ {fname} ({len(content):,} å­—ç¬¦)")

    full_text = "\n".join(parts)
    write_file(os.path.join(EXPORT_DIR, "knowledge_base_full.md"), full_text)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Storylines Full (æ•…äº‹çº¿ + å›¾æ‘˜è¦åˆå¹¶)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def export_storylines():
    print("\nğŸ­ ç”Ÿæˆ storylines_full.md ...")

    parts = []

    parts.append("# Omphalina æ•…äº‹çº¿æ•°æ® â€” ç¬¬äºŒå±‚ï¼šLLM åˆ›ä½œå±‚\n")
    parts.append("> å†…å®¹ï¼šSchema å®šä¹‰ + 7 åŒ–åˆç‰©æ•…äº‹çº¿ YAML + è·¨åŒ–åˆç‰©è¿æ¥ + å› æœå›¾æ‘˜è¦")
    parts.append("> ç”¨é€”ï¼šå™äº‹å¼•æ“çš„ç»“æ„åŒ–éª¨æ¶ï¼Œæ¯æ¡å› æœè¾¹å¸¦æœ‰ drama_score å’Œ irony_level è¯„åˆ†\n")
    parts.append("---\n")

    # â”€â”€ Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("# PART 1: æ•°æ®ç»“æ„å®šä¹‰ (Schema)\n")
    parts.append("```yaml")
    parts.append(read_file(os.path.join(STORYLINES_DIR, "schema.yaml")))
    parts.append("```\n")
    parts.append("---\n")
    print("    âœ“ schema.yaml")

    # â”€â”€ 7 ä¸ªåŒ–åˆç‰©æ•…äº‹çº¿ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("# PART 2: åŒ–åˆç‰©æ•…äº‹çº¿ (7 ç¯‡)\n")

    compound_order = ["aspirin", "synthetic_ammonia", "plastics", "ddt", "cfc", "penicillin", "msg"]
    compounds_dir = os.path.join(STORYLINES_DIR, "compounds")

    for cid in compound_order:
        filepath = os.path.join(compounds_dir, f"{cid}.yaml")
        if os.path.exists(filepath):
            content = read_file(filepath)
            parts.append(f"\n{'=' * 80}")
            parts.append(f"## æ•…äº‹çº¿ï¼š{cid}")
            parts.append(f"{'=' * 80}\n")
            parts.append("```yaml")
            parts.append(content)
            parts.append("```\n")
            parts.append("---\n")
            print(f"    âœ“ {cid}.yaml ({len(content):,} å­—ç¬¦)")

    # â”€â”€ è·¨åŒ–åˆç‰©è¿æ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("# PART 3: è·¨åŒ–åˆç‰©å› æœè¿æ¥\n")
    parts.append("```yaml")
    parts.append(read_file(os.path.join(STORYLINES_DIR, "cross_connections.yaml")))
    parts.append("```\n")
    parts.append("---\n")
    print("    âœ“ cross_connections.yaml")

    # â”€â”€ å› æœå›¾ç»Ÿè®¡æ‘˜è¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    parts.append("# PART 4: å› æœå›¾ç»Ÿè®¡æ‘˜è¦\n")
    parts.append(build_graph_summary())
    parts.append("\n---\n")
    print("    âœ“ graph summary (ä» graph_export.json æå–)")

    full_text = "\n".join(parts)
    write_file(os.path.join(EXPORT_DIR, "storylines_full.md"), full_text)


def build_graph_summary():
    """ä» graph_export.json æå–å…³é”®ç»Ÿè®¡ï¼Œé¿å…ä¸Šä¼  3161 è¡ŒåŸå§‹ JSON"""
    json_path = os.path.join(STORYLINES_DIR, "graph_export.json")
    if not os.path.exists(json_path):
        return "> âš ï¸ graph_export.json æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ graph_builder.py\n"

    with open(json_path, "r", encoding="utf-8") as f:
        graph_data = json.load(f)

    nodes = graph_data.get("nodes", [])
    links = graph_data.get("links", [])

    # èŠ‚ç‚¹ç»Ÿè®¡
    node_types = {}
    for n in nodes:
        nt = n.get("node_type", "Unknown")
        node_types[nt] = node_types.get(nt, 0) + 1

    # è¾¹ç»Ÿè®¡
    edge_types = {}
    high_drama = []
    high_irony = []

    for e in links:
        et = e.get("edge_type", "Unknown")
        edge_types[et] = edge_types.get(et, 0) + 1

        drama = e.get("drama_score", 0)
        irony = e.get("irony_level", 0)

        source_id = e.get("source", "?")
        target_id = e.get("target", "?")

        # æ‰¾åˆ°èŠ‚ç‚¹å
        source_name = source_id
        target_name = target_id
        for n in nodes:
            if n.get("id") == source_id:
                source_name = n.get("name_zh", n.get("description_zh", source_id))
            if n.get("id") == target_id:
                target_name = n.get("name_zh", n.get("description_zh", target_id))

        desc = e.get("description_zh", e.get("relationship_zh", ""))

        if drama and drama >= 0.85:
            high_drama.append({
                "source": source_name, "target": target_name,
                "drama": drama, "irony": irony or 0,
                "type": et, "desc": desc
            })
        if irony and irony >= 0.85:
            high_irony.append({
                "source": source_name, "target": target_name,
                "drama": drama or 0, "irony": irony,
                "type": et, "desc": desc
            })

    high_drama.sort(key=lambda x: x["drama"], reverse=True)
    high_irony.sort(key=lambda x: x["irony"], reverse=True)

    lines = []
    lines.append("ä»¥ä¸‹æ˜¯ä» NetworkX å› æœå›¾å¯¼å‡ºçš„ç»Ÿè®¡æ‘˜è¦ï¼ˆåŸå§‹å›¾åŒ…å« 125 èŠ‚ç‚¹ã€132 è¾¹ï¼‰ï¼š\n")

    lines.append("## èŠ‚ç‚¹ç»Ÿè®¡")
    lines.append(f"- æ€»èŠ‚ç‚¹æ•°: **{len(nodes)}**")
    for nt, count in sorted(node_types.items()):
        lines.append(f"  - {nt}: {count}")

    lines.append(f"\n## è¾¹ç»Ÿè®¡")
    lines.append(f"- æ€»è¾¹æ•°: **{len(links)}**")
    for et, count in sorted(edge_types.items()):
        lines.append(f"  - {et}: {count}")

    lines.append(f"\n## é«˜æˆå‰§æ€§å› æœè¾¹ TOP {min(25, len(high_drama))} (drama_score â‰¥ 0.85)")
    lines.append("| # | æºèŠ‚ç‚¹ | â†’ | ç›®æ ‡èŠ‚ç‚¹ | Drama | Irony | ç±»å‹ | æè¿° |")
    lines.append("|---|--------|---|----------|-------|-------|------|------|")
    for i, e in enumerate(high_drama[:25], 1):
        desc_short = e["desc"][:50] + "â€¦" if len(e["desc"]) > 50 else e["desc"]
        lines.append(f"| {i} | {e['source']} | â†’ | {e['target']} | {e['drama']:.2f} | {e['irony']:.2f} | {e['type']} | {desc_short} |")

    lines.append(f"\n## é«˜è®½åˆºæ€§å› æœè¾¹ TOP {min(20, len(high_irony))} (irony_level â‰¥ 0.85)")
    lines.append("| # | æºèŠ‚ç‚¹ | â†’ | ç›®æ ‡èŠ‚ç‚¹ | Irony | Drama | ç±»å‹ | æè¿° |")
    lines.append("|---|--------|---|----------|-------|-------|------|------|")
    for i, e in enumerate(high_irony[:20], 1):
        desc_short = e["desc"][:50] + "â€¦" if len(e["desc"]) > 50 else e["desc"]
        lines.append(f"| {i} | {e['source']} | â†’ | {e['target']} | {e['irony']:.2f} | {e['drama']:.2f} | {e['type']} | {desc_short} |")

    return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¸»æµç¨‹
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
def main():
    print("=" * 60)
    print("Omphalina â€” çŸ¥è¯†åº“å¯¼å‡º (é¢å‘ Google AI Studio)")
    print("=" * 60)

    export_system_instructions()
    export_knowledge_base()
    export_storylines()

    # æœ€ç»ˆæ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“¦ å¯¼å‡ºå®Œæˆï¼æ–‡ä»¶ä½äº:")
    print(f"   {EXPORT_DIR}/")

    total_chars = 0
    for fname in ["system_instructions.txt", "knowledge_base_full.md", "storylines_full.md"]:
        fpath = os.path.join(EXPORT_DIR, fname)
        if os.path.exists(fpath):
            size = os.path.getsize(fpath)
            total_chars += size

    print(f"\n   æ€»è®¡: {total_chars:,} å­—èŠ‚ (~{total_chars // 4:,} tokens)")
    print(f"   Gemini 1M ä¸Šä¸‹æ–‡å æ¯”: ~{total_chars // 4 / 10000:.1f}%")

    print(f"""
ğŸ“‹ AI Studio æ“ä½œæ­¥éª¤:
   1. æ‰“å¼€ aistudio.google.com â†’ æ–°å»º Chat
   2. å°† system_instructions.txt ç²˜è´´åˆ° System Instructions æ 
   3. ä¸Šä¼  knowledge_base_full.md å’Œ storylines_full.md ä½œä¸ºä¸Šä¸‹æ–‡
   4. åœ¨èŠå¤©æ¡†å‘é€å¼€åœº prompt
""")


if __name__ == "__main__":
    main()
