#!/usr/bin/env python3
"""
Omphalina çŸ¥è¯†åº“ â€” Wikipedia å…¨æ™¯çŸ¥è¯†æ‹‰å–è„šæœ¬

ä» Wikipedia è‹±æ–‡ç‰ˆå’Œä¸­æ–‡ç‰ˆæ‹‰å–åŒ–åˆç‰©å®Œæ•´æ¡ç›®ï¼Œ
æŒ‰ç« èŠ‚é‡ç»„ä¸º Markdown æ ¼å¼ï¼Œå­˜å…¥ encyclopedia/compounds/ã€‚
åŒæ—¶æ‹‰å–ç‹¬ç«‹å†å²ä¸“é¢˜æ¡ç›®å’Œå…³é”®äººç‰©ä¼ è®°ã€‚
"""

import os
import json
import time
from datetime import datetime

import wikipediaapi

# ============================================================
# é…ç½®
# ============================================================

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
COMPOUNDS_DIR = os.path.join(BASE_DIR, "encyclopedia", "compounds")
TOPICS_DIR = os.path.join(BASE_DIR, "encyclopedia", "topics")
PEOPLE_DIR = os.path.join(BASE_DIR, "encyclopedia", "people")
METADATA_PATH = os.path.join(BASE_DIR, "encyclopedia", "metadata.json")

USER_AGENT = "OmphalinaKnowledgeBase/1.0 (hackathon-project)"

# ä»£ç†é…ç½®ï¼ˆå¦‚éœ€ä»£ç†è¯·è®¾ç½® HTTP_PROXY / HTTPS_PROXY ç¯å¢ƒå˜é‡ï¼‰
PROXY = os.environ.get("HTTP_PROXY", os.environ.get("HTTPS_PROXY", ""))

# 7 ä¸ª MVP åŒ–åˆç‰©ï¼š(æ–‡ä»¶å, è‹±æ–‡ Wikipedia æ ‡é¢˜, ä¸­æ–‡ Wikipedia æ ‡é¢˜)
COMPOUNDS = [
    ("aspirin",             "Aspirin",                  "é˜¿å¸åŒ¹æ—"),
    ("synthetic_ammonia",   "Haber process",            "å“ˆæŸæ³•"),
    ("plastics",            "Plastic",                  "å¡‘æ–™"),
    ("ddt",                 "DDT",                      "æ»´æ»´æ¶•"),
    ("cfc",                 "Chlorofluorocarbon",       "æ°¯æ°Ÿçƒƒ"),
    ("penicillin",          "Penicillin",               "é’éœ‰ç´ "),
    ("msg",                 "Monosodium glutamate",     "å‘³ç²¾"),
]

# ç‹¬ç«‹å†å²ä¸“é¢˜æ¡ç›®
TOPICS = [
    ("history_of_aspirin",      "History of aspirin",       None),
    ("history_of_penicillin",   "History of penicillin",    None),
    ("silent_spring",           "Silent Spring",            "å¯‚é™çš„æ˜¥å¤©"),
    ("montreal_protocol",       "Montreal Protocol",        "è’™ç‰¹åˆ©å°”è®®å®šä¹¦"),
    ("haber_process_history",   "Fritz Haber",              "å¼—é‡ŒèŒ¨Â·å“ˆä¼¯"),
]

# å…³é”®äººç‰©ä¼ è®°
PEOPLE = [
    ("fritz_haber",         "Fritz Haber",              "å¼—é‡ŒèŒ¨Â·å“ˆä¼¯"),
    ("rachel_carson",       "Rachel Carson",            "è•¾åˆ‡å°”Â·å¡æ£®"),
    ("alexander_fleming",   "Alexander Fleming",        "äºšå†å±±å¤§Â·å¼—è±æ˜"),
    ("thomas_midgley",      "Thomas Midgley Jr.",       "å°æ‰˜é©¬æ–¯Â·ç±³å¥‡åˆ©"),
    ("paul_mueller",        "Paul Hermann MÃ¼ller",      None),
    ("kikunae_ikeda",       "Kikunae Ikeda",            "æ± ç”°èŠè‹—"),
    ("felix_hoffmann",      "Felix Hoffmann",           None),
    ("carl_bosch",          "Carl Bosch",               "å¡å°”Â·åšæ–½"),
]

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def create_wiki(lang: str) -> wikipediaapi.Wikipedia:
    """åˆ›å»ºæŒ‡å®šè¯­è¨€çš„ Wikipedia API å®¢æˆ·ç«¯ï¼ˆé€šè¿‡ä»£ç†ï¼‰"""
    wiki = wikipediaapi.Wikipedia(
        user_agent=USER_AGENT,
        language=lang,
        extract_format=wikipediaapi.ExtractFormat.WIKI,
        proxies={
            "http": PROXY,
            "https": PROXY,
        },
    )
    return wiki


def extract_sections_recursive(sections, level=2) -> str:
    """é€’å½’æå–æ‰€æœ‰å­ç« èŠ‚ï¼Œè½¬æ¢ä¸º Markdown æ ¼å¼"""
    md = ""
    for s in sections:
        # è·³è¿‡å‚è€ƒæ–‡çŒ®ã€å¤–éƒ¨é“¾æ¥ç­‰éå†…å®¹ç« èŠ‚
        skip_titles = {
            "References", "External links", "See also", "Further reading",
            "Notes", "Bibliography", "Sources", "Footnotes",
            "å‚è€ƒæ–‡çŒ®", "å¤–éƒ¨é“¾æ¥", "å‚è§", "å»¶ä¼¸é˜…è¯»", "æ³¨é‡Š", "å‚è€ƒèµ„æ–™",
        }
        if s.title in skip_titles:
            continue

        heading = "#" * level
        md += f"\n{heading} {s.title}\n\n"
        if s.text.strip():
            md += s.text.strip() + "\n"
        # é€’å½’å­ç« èŠ‚
        if s.sections:
            md += extract_sections_recursive(s.sections, level=level + 1)
    return md


def fetch_article(wiki, title: str) -> dict | None:
    """
    æ‹‰å–å•ç¯‡ Wikipedia æ–‡ç« ï¼Œè¿”å›ç»“æ„åŒ–æ•°æ®ã€‚
    è¿”å› None å¦‚æœæ–‡ç« ä¸å­˜åœ¨ã€‚
    """
    page = wiki.page(title)
    if not page.exists():
        print(f"  âš ï¸ æ–‡ç« ä¸å­˜åœ¨: {title}")
        return None

    return {
        "title": page.title,
        "summary": page.summary,
        "full_url": page.fullurl,
        "sections_md": extract_sections_recursive(page.sections),
        "text_length": len(page.text),
    }


def build_compound_md(filename: str, en_title: str, zh_title: str | None,
                      wiki_en, wiki_zh) -> tuple[str, dict]:
    """
    æ„å»ºä¸€ä¸ªåŒ–åˆç‰©çš„å®Œæ•´ Markdown ç™¾ç§‘æ¡ç›®ã€‚
    è¿”å› (markdown_text, metadata_dict)ã€‚
    """
    print(f"\nğŸ“– æ‹‰å–åŒ–åˆç‰©: {en_title}")

    # æ‹‰å–è‹±æ–‡
    en_data = fetch_article(wiki_en, en_title)
    if not en_data:
        return "", {}

    # æ‹‰å–ä¸­æ–‡
    zh_data = None
    if zh_title:
        zh_data = fetch_article(wiki_zh, zh_title)

    # ç»„è£… Markdown
    display_name_zh = zh_title or en_title
    md = f"# {display_name_zh} ({en_title})\n\n"
    md += f"> æ¥æº: Wikipedia EN + ZH | æ‹‰å–æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}\n"
    md += f"> EN: {en_data['full_url']}\n"
    if zh_data:
        md += f"> ZH: {zh_data['full_url']}\n"
    md += "\n---\n\n"

    # æ‘˜è¦
    md += "## æ‘˜è¦ (Summary)\n\n"
    md += f"**English:**\n{en_data['summary']}\n\n"
    if zh_data:
        md += f"**ä¸­æ–‡:**\n{zh_data['summary']}\n\n"

    # è‹±æ–‡æ­£æ–‡ï¼ˆæŒ‰ç« èŠ‚ï¼‰
    md += "---\n\n"
    md += "## è‹±æ–‡ Wikipedia å…¨æ–‡\n"
    md += en_data["sections_md"]

    # ä¸­æ–‡æ­£æ–‡ï¼ˆè¡¥å……ï¼‰
    if zh_data and zh_data["sections_md"].strip():
        md += "\n---\n\n"
        md += "## ä¸­æ–‡ Wikipedia è¡¥å……å†…å®¹\n"
        md += zh_data["sections_md"]

    # å…ƒæ•°æ®
    meta = {
        "filename": filename,
        "en_title": en_title,
        "zh_title": zh_title,
        "en_url": en_data["full_url"],
        "zh_url": zh_data["full_url"] if zh_data else None,
        "en_chars": en_data["text_length"],
        "zh_chars": zh_data["text_length"] if zh_data else 0,
        "total_chars": en_data["text_length"] + (zh_data["text_length"] if zh_data else 0),
        "estimated_tokens": int((en_data["text_length"] + (zh_data["text_length"] if zh_data else 0)) * 0.35),
        "fetched_at": datetime.now().isoformat(),
    }

    return md, meta


def build_article_md(en_title: str, zh_title: str | None,
                     wiki_en, wiki_zh, category: str) -> tuple[str, dict]:
    """
    æ„å»ºä¸“é¢˜/äººç‰©æ¡ç›®çš„ Markdownã€‚
    """
    print(f"\nğŸ“– æ‹‰å–{category}: {en_title}")

    en_data = fetch_article(wiki_en, en_title)
    if not en_data:
        return "", {}

    zh_data = None
    if zh_title:
        zh_data = fetch_article(wiki_zh, zh_title)

    display_name = zh_title or en_title
    md = f"# {display_name} ({en_title})\n\n"
    md += f"> æ¥æº: Wikipedia | ç±»åˆ«: {category} | æ‹‰å–æ—¥æœŸ: {datetime.now().strftime('%Y-%m-%d')}\n"
    md += f"> EN: {en_data['full_url']}\n"
    if zh_data:
        md += f"> ZH: {zh_data['full_url']}\n"
    md += "\n---\n\n"

    md += "## æ‘˜è¦\n\n"
    md += f"{en_data['summary']}\n\n"
    if zh_data:
        md += f"**ä¸­æ–‡æ‘˜è¦:**\n{zh_data['summary']}\n\n"

    md += "---\n\n"
    md += "## è‹±æ–‡å…¨æ–‡\n"
    md += en_data["sections_md"]

    if zh_data and zh_data["sections_md"].strip():
        md += "\n---\n\n"
        md += "## ä¸­æ–‡è¡¥å……\n"
        md += zh_data["sections_md"]

    meta = {
        "en_title": en_title,
        "zh_title": zh_title,
        "en_url": en_data["full_url"],
        "zh_url": zh_data["full_url"] if zh_data else None,
        "en_chars": en_data["text_length"],
        "zh_chars": zh_data["text_length"] if zh_data else 0,
        "total_chars": en_data["text_length"] + (zh_data["text_length"] if zh_data else 0),
        "estimated_tokens": int((en_data["text_length"] + (zh_data["text_length"] if zh_data else 0)) * 0.35),
        "fetched_at": datetime.now().isoformat(),
        "category": category,
    }

    return md, meta


# ============================================================
# ä¸»æµç¨‹
# ============================================================

def main():
    print("=" * 60)
    print("Omphalina çŸ¥è¯†åº“ â€” Wikipedia å…¨æ™¯æ‹‰å–")
    print("=" * 60)

    wiki_en = create_wiki("en")
    wiki_zh = create_wiki("zh")

    all_metadata = {
        "project": "Omphalina",
        "description": "åŒ–åˆç‰©å…¨æ™¯çŸ¥è¯†åº“ â€” ç¬¬ä¸€å±‚ç™¾ç§‘æ•°æ®",
        "generated_at": datetime.now().isoformat(),
        "compounds": {},
        "topics": {},
        "people": {},
        "totals": {},
    }

    total_chars = 0
    total_tokens = 0

    # --- æ‹‰å– 7 ä¸ªåŒ–åˆç‰© ---
    print("\n" + "=" * 40)
    print("ç¬¬ä¸€æ­¥ï¼šæ‹‰å– 7 ä¸ªåŒ–åˆç‰©ç™¾ç§‘æ¡ç›®")
    print("=" * 40)

    for filename, en_title, zh_title in COMPOUNDS:
        md, meta = build_compound_md(filename, en_title, zh_title, wiki_en, wiki_zh)
        if md:
            filepath = os.path.join(COMPOUNDS_DIR, f"{filename}.md")
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(md)
            print(f"  âœ… å·²ä¿å­˜: {filepath}")
            print(f"     å­—ç¬¦æ•°: {meta['total_chars']:,} | ä¼°ç®— tokens: {meta['estimated_tokens']:,}")
            all_metadata["compounds"][filename] = meta
            total_chars += meta["total_chars"]
            total_tokens += meta["estimated_tokens"]
        time.sleep(1)  # ç¤¼è²Œå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«

    # --- æ‹‰å–ä¸“é¢˜æ¡ç›® ---
    print("\n" + "=" * 40)
    print("ç¬¬äºŒæ­¥ï¼šæ‹‰å–ä¸“é¢˜è¡¥å……æ¡ç›®")
    print("=" * 40)

    for filename, en_title, zh_title in TOPICS:
        md, meta = build_article_md(en_title, zh_title, wiki_en, wiki_zh, "ä¸“é¢˜")
        if md:
            filepath = os.path.join(TOPICS_DIR, f"{filename}.md")
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(md)
            print(f"  âœ… å·²ä¿å­˜: {filepath}")
            all_metadata["topics"][filename] = meta
            total_chars += meta.get("total_chars", 0)
            total_tokens += meta.get("estimated_tokens", 0)
        time.sleep(1)

    # --- æ‹‰å–äººç‰©ä¼ è®° ---
    print("\n" + "=" * 40)
    print("ç¬¬ä¸‰æ­¥ï¼šæ‹‰å–å…³é”®äººç‰©ä¼ è®°")
    print("=" * 40)

    for filename, en_title, zh_title in PEOPLE:
        md, meta = build_article_md(en_title, zh_title, wiki_en, wiki_zh, "äººç‰©ä¼ è®°")
        if md:
            filepath = os.path.join(PEOPLE_DIR, f"{filename}.md")
            with open(filepath, "w", encoding="utf-8") as f:
                f.write(md)
            print(f"  âœ… å·²ä¿å­˜: {filepath}")
            all_metadata["people"][filename] = meta
            total_chars += meta.get("total_chars", 0)
            total_tokens += meta.get("estimated_tokens", 0)
        time.sleep(1)

    # --- æ±‡æ€»ç»Ÿè®¡ ---
    all_metadata["totals"] = {
        "total_chars": total_chars,
        "total_estimated_tokens": total_tokens,
        "gemini_context_usage_pct": round(total_tokens / 1_000_000 * 100, 1),
        "compound_count": len(all_metadata["compounds"]),
        "topic_count": len(all_metadata["topics"]),
        "people_count": len(all_metadata["people"]),
    }

    # ä¿å­˜å…ƒæ•°æ®
    with open(METADATA_PATH, "w", encoding="utf-8") as f:
        json.dump(all_metadata, f, ensure_ascii=False, indent=2)

    # æ‰“å°æ±‡æ€»
    print("\n" + "=" * 60)
    print("ğŸ“Š æ‹‰å–å®Œæˆï¼æ±‡æ€»ç»Ÿè®¡:")
    print("=" * 60)
    print(f"  åŒ–åˆç‰©æ¡ç›®: {all_metadata['totals']['compound_count']}")
    print(f"  ä¸“é¢˜æ¡ç›®:   {all_metadata['totals']['topic_count']}")
    print(f"  äººç‰©ä¼ è®°:   {all_metadata['totals']['people_count']}")
    print(f"  æ€»å­—ç¬¦æ•°:   {total_chars:,}")
    print(f"  ä¼°ç®— Token: {total_tokens:,}")
    print(f"  Gemini ä¸Šä¸‹æ–‡å æ¯”: ~{all_metadata['totals']['gemini_context_usage_pct']}%")
    print(f"\n  å…ƒæ•°æ®å·²ä¿å­˜: {METADATA_PATH}")
    print("=" * 60)


if __name__ == "__main__":
    main()
